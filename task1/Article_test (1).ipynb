{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-CVx6_PvP8c"
   },
   "source": [
    "## Import all modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "BKoLXhZzisny",
    "outputId": "f36ce9a1-8ae2-4af3-9d8f-055ba9ad93ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cpu)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (20.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.7.0\n",
      "    Uninstalling fsspec-2025.7.0:\n",
      "      Successfully uninstalled fsspec-2025.7.0\n",
      "Successfully installed datasets-4.0.0 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install transformers datasets accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "SFuDky49pig9",
    "outputId": "3d375520-1e32-4051-a4f1-6784fc1c3fb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.1.2\n",
      "  Downloading torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (4.14.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.2)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.2)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.2)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.2)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.2)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.2)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.2)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.2)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.2)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.2)\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.2)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.1.0 (from torch==2.1.2)\n",
      "  Downloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.2) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.2) (1.3.0)\n",
      "Downloading torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m546.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m118.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0+cpu\n",
      "    Uninstalling torch-2.6.0+cpu:\n",
      "      Successfully uninstalled torch-2.6.0+cpu\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.21.0+cpu requires torch==2.6.0, but you have torch 2.1.2 which is incompatible.\n",
      "torchaudio 2.6.0+cpu requires torch==2.6.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 torch-2.1.2 triton-2.1.0\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.1.2\n",
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "wYSqzHoEqrN0",
    "outputId": "7e8c7f7e-bd29-45d4-b459-09aa308fe158"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.9.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.9.86)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.9.0-py3-none-any.whl (367 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.1/367.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.8.1\n",
      "    Uninstalling accelerate-1.8.1:\n",
      "      Successfully uninstalled accelerate-1.8.1\n",
      "Successfully installed accelerate-1.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "642eYxTdrHdf",
    "outputId": "dc85038a-27ed-4e48-a48b-c5a9e2d41c03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch-xla 2.6.0\n",
      "Uninstalling torch-xla-2.6.0:\n",
      "  Successfully uninstalled torch-xla-2.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y torch_xla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "amVwz_varLGC",
    "outputId": "709f7e4c-353c-440e-8906-8231500141e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
      "Collecting torch\n",
      "  Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cpu)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cpu)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch)\n",
      "  Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m459.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl (7.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
      "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.86\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.86:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.86\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.18.1\n",
      "    Uninstalling nvidia-nccl-cu12-2.18.1:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.18.1\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
      "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
      "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
      "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
      "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
      "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.2\n",
      "    Uninstalling torch-2.1.2:\n",
      "      Successfully uninstalled torch-2.1.2\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.21.0+cpu\n",
      "    Uninstalling torchvision-0.21.0+cpu:\n",
      "      Successfully uninstalled torchvision-0.21.0+cpu\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.6.0+cpu\n",
      "    Uninstalling torchaudio-2.6.0+cpu:\n",
      "      Successfully uninstalled torchaudio-2.6.0+cpu\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.1 torchaudio-2.7.1 torchvision-0.22.1 triton-3.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txeUpZJds_iE"
   },
   "source": [
    "## HuggingFace LOGIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "27f4d67b2b914642b5a45ae8497569fb",
      "63d43377b8e041ee851e0afffa1adf61",
      "fff5aa1d294c4851a128166e09dc9a8f",
      "c5b96a0e015749089a914357b6183505",
      "8f6ad5984c15439bb86a7d27e40114bb",
      "d4f8561062fa4c4390b626bbc43fdc09",
      "cac5ac4f96584220b7e3eb8fa5e1ecc0",
      "350589b4191d4b9986caca9e1bc28315",
      "27e71555039f44f9b80ded6577d831f7",
      "678586d74d1147218bd8a97fd5ba19c6",
      "2352f4153d604d84b73400ccdc35934b",
      "08110c443bfa4b5e90dc04c479526e35",
      "8161f5c78bcc4970bf533cf0bf5b90be",
      "150464ffea8d4f03b3650d04da2ebd83",
      "c278526fa41341a6b61d71fe31aebea9",
      "9aed23b7252e4f66b7b2e61f302d8b59",
      "db84702fd65f442493f038446206637d",
      "42c46160db1e41c39005df9624e15f48",
      "6ff4d3828a064a28800be727a3745eb0",
      "523732c5edca457c8d15e442a0ab2f5c"
     ]
    },
    "id": "06c4ab62",
    "outputId": "1a5df19a-8ae3-4bd1-b281-3b7b4018f820"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f4d67b2b914642b5a45ae8497569fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "login = \"add login token here\"\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqMm3CCcspjY"
   },
   "source": [
    "Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcseCg4Xo7eI"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "prompt = \"User: Write a detailed article on how AI is transforming education in 2025.\\nAssistant:\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AeWraT-9skRZ"
   },
   "source": [
    "## Mistral 7B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 566,
     "referenced_widgets": [
      "125d1e0cb95e4df09e01f18a4483b4ef",
      "1c81c1c462af40d29ce80b97578db694",
      "b1901ea18a9248b5968e6fa8c790e652",
      "9f994b6939e34c13b5ad9ea669c9bc5d",
      "915468b3e71446dcb591ca87ef659b29",
      "a472d175f8fa4bd8a3608b8bb9016e9a",
      "d556b84b38294ef0814dd4e42f27d098",
      "715da4ae8c3540718b9a97d594d5c57e",
      "ca218c42b8784f37b7f3e0ace0fd60e6",
      "c164719da8794f9ea67d7e9e42fe7820",
      "0caf21282f0f4841b947e4e440de2b7e",
      "afda5cf22aad4391b127b3b7ec29e773",
      "e8b576cea357419dbcc0e653f8a27a7b",
      "254b0772930743a2959dde54de729f32",
      "3696e0a53afa4d99a4b0d5df33fb8067",
      "3ed1c1ce09f54847bd33cbbb4f0c544b",
      "d133ce0ff8104c53a107b105b9c1e4ea",
      "98eb0ee67d2140a09c83fdc0f05f958f",
      "d1159c08997e462f8334aba235404817",
      "7906aba64286464ab8e1827315fc9551",
      "1d67dae942d64edfba9637b8b6c8cf1e",
      "1a5cd465389d49d5b1885926357c4f59",
      "9e24f2f54035411bbee6cdd4f29418c9",
      "165d1b64531242c7aeeb6d3285231011",
      "aec014cb67d5498e842ae70daed84dc0",
      "3d74d1423ddb4097be3519fe7474eaee",
      "26e5d01135e6417c86ca0bbdc873caac",
      "76a9d52919564fcbbf08a18daeb52768",
      "d2750f2ad3a742e3bd9d4762f7340d3b",
      "d268a8fec31846f4805ada6e18fa0e6a",
      "7701c694914340619896879375b5979d",
      "4f00653226924d01a3a030ba51697a66",
      "fd651a0f06534e8d88c75a64dbc9ddbf",
      "ea3da068527c409b848f4b9b2c14ab15",
      "af571fd9b10b4d5ebcabb2e43823d7b5",
      "aeb0214218364385b96d7e3b3f67c30b",
      "e708959c2dd54f1cb44e6f1a56866284",
      "dd01248e76104aac99730785869da19e",
      "5fda93040fe144ab949826baa24edb48",
      "e09429ab8c8140bcb0cca5582f375ebb",
      "6b36d3271a744574918b8301ad204efe",
      "0decb24e2d0a4ea0baacb1b371536cb1",
      "51637f7532aa451d95994fde52b57ddd",
      "79e434a859f846139b9beb6bbb32fc17",
      "32065566a284456d82eadc408b75ebf7",
      "38cd0bebbada4b118aef6630bcd54043",
      "29e56d15cae14b4690a25043a0f619c9",
      "359aa5e5b31745e9b7325c9b87071b62",
      "84647ab4af2842ea87af3dfb440476e2",
      "4f0be0fab87042b6b7e183e99341b693",
      "f75d1d61e60540509a70639eedc49e04",
      "3e9916f879784d66a32eb5581d31eccf",
      "9182d393147c432988dfe62e15d90fe9",
      "52887d9b78fa427c8ce5847f2111c37f",
      "2ab96aaa954a4f10b7559c8d52e886fe",
      "0a9eaabe24034be58fe7e7d3198bc657",
      "e86ec4f6e87b4bda9337630d980026f3",
      "dc6b987549304db79ce14fe7f2c163c0",
      "c1369e5b949445cea173cc98e8f0dafb",
      "f55ff2251b6c4223aaf4806b6e071b6e",
      "a8fe6209b4ec42cdb27e997938c37191",
      "6af233b57b094ab6bafdde1f49bc269c",
      "02a13c00e53648429d43c61dc04f545a",
      "96db8484609849a3aa514fc42279e7ff",
      "6aee342feda540be8a815e8ce85f905a",
      "8d61d6b2dc7d4499af41294bab6911e4",
      "847962a401db43e8bfde2282dbfd7b06",
      "93a2d74be7394cf8bf6945965af534f2",
      "199fca586e07436f9f95de0603d2a301",
      "3ca648e552f74124aa303d666d97e08a",
      "130f1666a4294e1ca79797574463a4cf",
      "3f416284828a42e8a41094d63cd754cb",
      "a50049039066431ca2d3eb8a624b91e5",
      "1e5a9333322148fe8b2fb08a99a8711b",
      "2ef733fff0f940bc93e33a0fefc684bd",
      "5ceab701720c44f2a691143aa79ce519",
      "0b356917cf8d41d48476eb98d217ffb3",
      "b6d8438c9b0042d08b8165e5703d0c20",
      "de4eae29ce594a7d99217e4ea47cef72",
      "98a7b80d635a4410993600c084815c96",
      "4cb551751945437785d06a3d2023931b",
      "fb9f8bb37e0249ecb6336abb02f6b17d",
      "ab98fca4c7f64629b92d10339a2cf70f",
      "3cdea5445b9d4166bf4b41db85501fb9",
      "dca53f6c2d93407ba0bbc0da8c51b222",
      "e6e2923c03f142de8bace812514ffff2",
      "ffde992ae3ff4d7687b0af48bf412c5d",
      "7a56552da22e4c519aaa8c194d9bd776",
      "384e435656974de1825f98f709212757",
      "d5efab595dfa4d40938e3b2dc616c717",
      "d60f97aedd9a4996b7b3db0abbe5fdf1",
      "54976732e3a04b9fa0ff0221f80c542a",
      "16f506ddb1a84c629c6c676dc08bcda3",
      "132050e368154472b4c95048cb406078",
      "ed673ef9696d4fa586342c4f69e836cd",
      "b944468ff296452e8122b677dcf4ebf7",
      "bfd1e2d167d1414fa6f86ec313c5f383",
      "f75cd686d84c45f3b77a4df5c2e06783",
      "c5c0d7c0fb374fefb7978ab24c572293",
      "b735e06c558042bf980a16cfe614fafc",
      "f7c38c60584e4d8aa2e29b448cdafe01",
      "46da8b029fa040f49017d1d2c5d3a7ae",
      "8e715afc0629424d88fda91273302164",
      "3efa7aa8acd74e7cbd42825990ffb671",
      "303221ee36404bd596bb5e65afdbdde1",
      "04cf6d934b79413c8a053eee749f9b95",
      "67c642d014e9482baeaabb9ed54a2224",
      "664a13c8be6f4d418c7d39a5e54d9cc6",
      "56f91e91dde747c1ac4c787cfe1e4f6a",
      "e78a9efa7a7e4e16ba7692b83d18bc92",
      "bfc66011eb094e5a9389bf436cfbeb44",
      "70d5f0d34f1b4bc990d676c55d5761bb",
      "db94896b5a6b49d4834f2642c27302de",
      "c1f3e201e66848a6bb1c9079f38066b8",
      "1f78d5e546ec4d73aecbff33551b4837",
      "5ac2c2afc256472c9c432094df503646",
      "3c152b3227514cb88e00eecb733650ae",
      "399f06f2b88a4c66bf6360c19e2b8623",
      "3a6d66531022429ca9306cdf711eb612",
      "5ce2265f996e4c078348996c50d148d8",
      "4559fb1e140d4dc5b623604899e37726"
     ]
    },
    "id": "FbtxoEPsUVpb",
    "outputId": "19b57798-b231-444b-87d5-b1b23b0cd71c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125d1e0cb95e4df09e01f18a4483b4ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afda5cf22aad4391b127b3b7ec29e773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e24f2f54035411bbee6cdd4f29418c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3da068527c409b848f4b9b2c14ab15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32065566a284456d82eadc408b75ebf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9eaabe24034be58fe7e7d3198bc657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847962a401db43e8bfde2282dbfd7b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d8438c9b0042d08b8165e5703d0c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384e435656974de1825f98f709212757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b735e06c558042bf980a16cfe614fafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc66011eb094e5a9389bf436cfbeb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model with attention implementation: eager\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_mistral_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "tokenizer_mistral = AutoTokenizer.from_pretrained(model_mistral_name)\n",
    "model_mistral = AutoModelForCausalLM.from_pretrained(model_mistral_name, attn_implementation=\"eager\")\n",
    "\n",
    "print(f\"Loaded model with attention implementation: {model_mistral.config._attn_implementation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4y1TFE5iZRa2",
    "outputId": "9d043650-249c-4afa-f338-042b1804b2e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Generated Text Mistral\n",
      "User: Write a detailed article on how AI is transforming education in 2025.\n",
      "Assistant: Ok, let’s start by defining what AI (Artificial Intelligence) is and what it means for education. AI refers to the simulation of human intelligence in machines that are programmed to think, learn, and solve problems like humans do. In education, AI has the potential to transform the way we teach and learn by providing personalized learning experiences, improving student engagement, assessing students’ progress, providing feedback in real-time, enhancing classroom management, reducing administrative tasks, creating more inclusive learning environments, promoting lifelong learning, etc.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "generator = pipeline(\"text-generation\", model=model_mistral, tokenizer=tokenizer_mistral)\n",
    "output = generator(prompt, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n",
    "generated_text = output[0]['generated_text']\n",
    "\n",
    "print(\"\\n\\n Generated Text Mistral\")\n",
    "print(generated_text) # Use print to display the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wd5d3U5CYs4K"
   },
   "source": [
    "**Evaluation of Text (Mistral):**\n",
    "The text is the most relevant, accurate, factual etc. Though it doesnt start as an article, it takes the prompt as an AI assistant therefore it is accurate in that way. Great vocabulary and grammar. This text gave the most relevant answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "or-5taOHsu7-"
   },
   "source": [
    "## LLAMA3 8B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530,
     "referenced_widgets": [
      "2ee0dde260c441cdb35085532017020c",
      "e43bfda63e2647fd82f87b2a440265df",
      "354e538e45d64c0897b0b82aea220819",
      "cde1e35b89534a319cacdb28f268741e",
      "fe3bb38d5e504a8cbda4ef5388aaa98d",
      "b21a6183acd44522bc0471ef126194a2",
      "5e695ad138244c02b94d0a8c3d419f30",
      "fda84d33d8f648aa9d51c7d56f893ad6",
      "dc33b4bea02c425d9476a5c443f20f67",
      "c4d525144a904427929ef14fe9a0aa5b",
      "405d86e734ea4ac2af78d3cab5c00ac2",
      "49065810619d421fac2c358fb39a976a",
      "940402cc8d294ba4b67eb5eeb86fa9a2",
      "3ab5d9979c034f5387ce29a2b3fc91f2",
      "c616bcb10af3482ebe9987569e30a3b8",
      "85d68bfa94714788be7e119e2c5de2a6",
      "5c12bb960f44432d9039a6c3a1a09d5f",
      "2ea3ae620f1a496a877283604144c597",
      "4216f468b99647d4b29e268856da530a",
      "6d82cfa521c640d88dc9cf042b06a597",
      "51f4d58a2d9e48979ee97b42ea1a5bd9",
      "200cb28e6774441cb83acc9f00282e03",
      "2570a991d27f4a85ac0b3e59aaf2826f",
      "8ab96a2c9e7b49629422c756c59f8885",
      "650a4bfb51604efd986884c238019dc3",
      "3eedd7f5a28d4ee8b9dc5dff579fc860",
      "913a6d81beb245a193aaafa26288eedc",
      "74aa6cf6c1cb480fa4447042ce910f0f",
      "d344a807bff34f88b845a07cc9f4550d",
      "68dec9ca65154117b886c37a4d6297c5",
      "41ebcf5ae29b4aa58852e119a3e30a3c",
      "d31d4df73b404821b4d86b4f16249d22",
      "a4c93367150c42d993ea0b3a21adf8a1",
      "47cf66ed27474f48a8bcf836f8373878",
      "7c4e44606ffa4e26b0783e8839212ae1",
      "69ec82840cbf412bb50ffbdc4c055514",
      "d82cb6523a464958bbef055011f20250",
      "b967f72b554a452b858d18954f2f76e2",
      "a2b0455d33124dc5a3c14556a72043fa",
      "c2859b14329a4db08d509ed2b0a3734a",
      "6fe3f33f6f324da8a2ff0b50a848e769",
      "27214b74d8774fa7a2763495b110b795",
      "4eb0b624ad9c49518de38f27ca4b68d5",
      "ac2eb783989947409105a47c1c1b4081",
      "2c30af5ae1f147cd9276ec1e03fb92a8",
      "f45a54ed33dd40409315e4df7e77f67c",
      "eb37c51e9f424e878b8080489664e4e6",
      "f4e16078dc0d4239a2e268ed3816b891",
      "50a8bec52a3b4b38a1d2e0fefdc489e4",
      "6f2737d4ed67409a9d99b7b6ce7f9fdf",
      "a9ea4ff6fdc44a3fb0db8f8d8d781270",
      "8b4e69c1c40443748e97230f296ac0a7",
      "6c28e851787c4232b2ef8243041c2dd0",
      "280e4257ed434f15bdefc93fdf6f0748",
      "c134bfc0efb247e9b94a7d2b4fa0c1e6",
      "ca8b7dcec0d7429f82c7d436780b0f65",
      "53dbefeb875a4c19a25728f71c5e4ea6",
      "c7102d629d7649ab8f5c6bc2d9ec1003",
      "5743b6e3fb9c4dba926111966a9f032a",
      "f286f429473c43f99a92c80be3abbd72",
      "fa2ce6fe169e4cf4bcc33e5ff59495b0",
      "31fb25fc5f9540c0b92246564ab2e712",
      "d138cbe4146a4b11a6fef6292cc926a7",
      "bd79cf27b6bf4e4bb2f104ffc73b0075",
      "7c66448df6424433a10e2a1e8dee4318",
      "f6a1fd9dadb24b46afa53d633c16a6ea",
      "0185b2c6fc7e48d4924c2e2ba7f3947f",
      "82208a5a54fc4173a7512c22b84fffbd",
      "390c2d81260843b6a5f30455b98b5b61",
      "714efbb20a45457681d431193ff72d19",
      "5590ac6f94104e56bb88f15acb9afb66",
      "0cc714dcc5af4dfaa4ffcffea21e6857",
      "e08ebb6e862245aaaabff85a6cea31d7",
      "604c0456ea81451a9918f240d6d2b80d",
      "1cde400e024247d6a4cff3afc7211d9e",
      "aec2e004117d4354a9cc4862f3f35071",
      "68cc9d250b584dcdb81265c0f7d29e61",
      "250911a1b97e425eb8924273b84f2d6d",
      "597eed3e251646dbb42bdb88c79e0124",
      "91f6cc7496dc48ddac82b6d613237e21",
      "178f51cfd58a42f5954be8a86d0ba25b",
      "25b9932fc7a34a4b93bcbd33c9468c85",
      "3639f0d740fc459eab75a53e9bdb5860",
      "fec1c01be72647fa976023ed3478e148",
      "3d79401d7e7e4520a11f0669a4bbe853",
      "5d74133f18bd4662a3a9588b491225e3",
      "5a1fc36260ff4e01be3f5025bd783691",
      "334d958ec1f54967a4d774388215b8b0",
      "748d6b42e6fd48e3b02056f9e8cbba37",
      "029cebee5b844e5a9840bd6f6a4f589a",
      "3c6b19fab0224c2096e63ff7844dd9bb",
      "b2330a9813f84915a0603b5a414fc160",
      "239b1426039b4ae5a9e6372d2eb4f369",
      "0a8f5116fcd54094b3e6050ae4eea43d",
      "8a6140a417ce499288f71bf62f89780a",
      "3cad7e8d446e40c9a4e92d09aa86e013",
      "cf65226bfaf84f40aabd9d40f60f9d01",
      "357da509e35f4cfaa21afd3718de9a7b",
      "25166e1d51364afea7b17f99c3bd06d7",
      "f832aa229dae47ee987f109a4775414f",
      "b7dce63302a34677a43a0409f307e867",
      "08ea24997a944f44a8cffa40dc67aecd",
      "ec6a5b5e622f4fa3bf297504b9e5fbb0",
      "b9a3e2a3ea864ec5b74e864ff11987e4",
      "a5c6dd68a94d47dca1cf16cfddaf0ec7",
      "2b192de7b662467983eb543fea6152c8",
      "fdeafdf6d616472993574862df46e29e",
      "fe1b3a630de44274b1102b6d7da7c941",
      "ddec1a59d7df4159a0bc4a8264d6b971",
      "331f5d000f314338bf5c4bcc4ae0a90d",
      "eb0b2b341591402fbdd3e4a723d4df25",
      "73bac10a73864ecba3d9b8131e70eb83",
      "c5c7d62709f8441f9b391ca28e81d52a",
      "7208a7d9afb1454c8073b3a022bfe2f5",
      "b94ea9c8457448668e1acb4498e159ea",
      "2ebd3408c56347169945a4e4558c99b2",
      "91aa2e8021d442f98a130fd10fade199",
      "e89975fdca6f4c82a2747504f88915be",
      "551ccbf39aa7413a80a08e771d4c82eb",
      "a60e250cd9024e5d9fdb0c4bd188a051",
      "5ea2a7bd20334c40936a1f7a66a30257",
      "2a976648a00b4ee08d9c505792a95b83",
      "cdff9b1a2ebc416a82724c5f9a01be97",
      "7f8cd255e21f42d9ab4af62212216887",
      "125993dd2d774617b7c4d75de0a950d2",
      "ec8f11e2cfac42a9a5c742a9f7dd038a",
      "98e5b68f8bb44adda639d7ea37f87b30",
      "c0de662d6cf14c8f8419b86cfcc1ba9b",
      "d01f5cb95a8f406581aaeb3c6b7858d7",
      "73a535cb86e146968fb5423198825621",
      "d94c4b54de6349eab765eed3bf54d2f9",
      "b1edc74308b941e1a1e4bbd6f0b8da01"
     ]
    },
    "id": "U-mf-_sHHHns",
    "outputId": "2a6aee2d-3f33-4283-f64c-b195f1008a37"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee0dde260c441cdb35085532017020c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49065810619d421fac2c358fb39a976a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2570a991d27f4a85ac0b3e59aaf2826f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47cf66ed27474f48a8bcf836f8373878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c30af5ae1f147cd9276ec1e03fb92a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8b7dcec0d7429f82c7d436780b0f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0185b2c6fc7e48d4924c2e2ba7f3947f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250911a1b97e425eb8924273b84f2d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748d6b42e6fd48e3b02056f9e8cbba37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f832aa229dae47ee987f109a4775414f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0b2b341591402fbdd3e4a723d4df25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a976648a00b4ee08d9c505792a95b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded meta-llama/Llama-3.1-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name_llama3 = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "try:\n",
    "     tokenizer_llama3 = AutoTokenizer.from_pretrained(model_name_llama3)\n",
    "     model_llama3 = AutoModelForCausalLM.from_pretrained(model_name_llama3)\n",
    "     print(f\"Successfully loaded {model_name_llama3}\")\n",
    "except Exception as e:\n",
    "     print(f\"Could not load {model_name_llama3}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0dU_ZXWEqAzq",
    "outputId": "a2b21d9f-a1b5-4875-afc5-4151fcd3072e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Generated Text LLAMA3\n",
      "User: Write a detailed article on how AI is transforming education in 2025.\n",
      "Assistant: Here is a comprehensive article detailing the impact of AI on education by 25.\n",
      "\n",
      "**The Revolutionary Impact of Artificial Intelligence on Education in the Year 2050**\n",
      "\n",
      "The year 2035 marked a significant turning point in human history, as artificial intelligence (AI) began to transform various sectors of society, including education. By 2045, AI had become an integral part of the educational landscape, and its impact continued to grow in strength and scope. As we approach the year of 2060, the effects of this technological revolution are more pronounced than ever. In this article, we will delve into the ways in which AI has transformed education, highlighting the benefits and challenges that come with this shift.\n",
      "\n",
      "### Personalized Learning Experience\n",
      "\n",
      "One of a major impact on AI in education is the creation of highly personalized learning experiences. AI-powered adaptive learning platforms analyze students' performance, interests, learning styles, pace, strengths, weaknesses, goals, motivation, peer-to-peer learning, engagement, personal preferences, aptitude, abilities, behavior, habits, emotions, knowledge gaps, content, instructional strategies, materials, tools, techniques, resources, activities, courses, programs, curriculum, syllabus, assignments, feedback, grading, assessment, evaluations, progress, outcomes, performance metrics, achievement,\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=model_llama3, tokenizer=tokenizer_llama3)\n",
    "output = generator(prompt, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n",
    "generated_text = output[0]['generated_text']\n",
    "\n",
    "print(\"\\n\\n Generated Text LLAMA3\")\n",
    "print(generated_text) # Use print to display the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZpfNPfsXVje"
   },
   "source": [
    "**Evaluation of Text (LLAMA3) :**\n",
    "The text is relevant and creative but it isnt in the context of an article but of a creative essay. It has good vocabulary, grammar and sentence formation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5S1IpibsyA4"
   },
   "source": [
    "## Falcon 7B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410,
     "referenced_widgets": [
      "df1ff710ac2347b5ade243b0d8bacb6a",
      "81eb0fb995e2481d80973bfcea2ef619",
      "0dc0f17f7b854874b18b2e63586e066e",
      "1985bfa9f2b34909bead4c8f4b104223",
      "50f8936c551647dab49e2d22e105ffd4",
      "9b6c96a4a40040a288c345893cb73bf2",
      "a19fb41ddc544bc3aa41a21576d5f448",
      "10d69f0a83634ca589d2435f948f7708",
      "14bed97cae20400fa448db0da32dc215",
      "55a7e95da53247968bed68e284f3208a",
      "d96756e5d9814403a0ee3738d860bf3f",
      "f959296a71d44da49bebf48162119c93",
      "e411b3605d4d434996708ea0c194e4fa",
      "0799c71e4ff14797b8ff148cb879e631",
      "d934cbde9abe4e64b7c00e882e040bdd",
      "4cfa00d4d5bb43f4ac2ff8c4325dbb58",
      "61ec17b5fb8e4911873d7cc99c8a9277",
      "cbf519df1db34ea0a0000a4a91d72cf2",
      "1fcb9ef502a44400b77aa29da2c275c6",
      "09a433c83e9949dcb6fea3115ff87783",
      "bd7c12d40e434c798bcfc1a9bbd43c10",
      "968a0ec6740940d1badc861da30fb8e0",
      "795bf7d61227473a9ba67ebe744338d4",
      "7310eb7121964afbb50f2720748dd0ee",
      "9463d719c47a4d88aa0faeedee1ada79",
      "42d884fde77840ea8db29e8fc2a59333",
      "d90f289a17a042a0b6d9018725346861",
      "7cb94ae19b154b189165c6d2942dbaed",
      "1c34d125940e4801bf35876d9a9348a6",
      "a9c627b3c1894f9ba1032cbc57fee610",
      "17e46c5b2cc6474299758f6e45c62cff",
      "503e9ffb23114b40a42a55636ef4b097",
      "c7ac0cc7be624360bf7ab6f7872b43df",
      "8479fd7c2dbc47319329ca5e9351f263",
      "57ac6d3e66324d1f8a3c9825b60a400b",
      "bdcd39fe29d4419e9c62c5a32c888085",
      "208fd183d9814f32ad953a4715e19a41",
      "66247092b73d4031a449e5c85030bfa3",
      "467c5317a2084ef084107eea9a1c86b1",
      "3d2855a247e44841a6f2a108b442a55d",
      "aba5ee9aaeaf44e3ab8c082bf97a6d09",
      "264fc17010164ad9a965219bff71a3ff",
      "e08e28984e654bd7b7c3d1d66377bd24",
      "020cb9ce80794190a0703696f1461fe2",
      "d953f58c168845fabe9b5185e3c6b0c1",
      "149064a13e7044068dc3da9e93cbf4f0",
      "687a052b3a70482da94a3a6ae5aab3d1",
      "200f20d923e8408cb8ade2278965da16",
      "71dac4205c3a4ec7a395153975904860",
      "5f95a76840724d299c84b5d531e18e23",
      "7a55877585d943fda0f3e29fddc1fccf",
      "5eeabc3066b04d0390d51bd82c478a21",
      "2f434f9b774d45919333a0063c6e4917",
      "2cd67353f8074a57a28f94978654bef4",
      "7ee91e37f32045d9b9975faeb8306cec",
      "ec514ade7add4498b722c77daaffc840",
      "7a7745dde99f41e0bac128293fab047e",
      "15c7a938ad714835a71ac75e62f13e15",
      "a6dd94597d214b0c8e6417c059eeae4f",
      "46a99da2d545417ab9372d12e80b2560",
      "09d937266d7c45e6a45158769a354213",
      "bf0fac46e3884b66894e33bb9afa0665",
      "1320eeb870424e28ba3077ce6e00a8e7",
      "5f5c03d079cb4701a59ba0584e3c174a",
      "06d85d2c4d1d4d739c2b21b192d0cf6b",
      "e0451d9eb40d490eb37850088ea37943",
      "98e65dd3829f4a8d9439a686865b865f",
      "694f4dba9bc841e7983ece523097996b",
      "a0f22a437bd14251b1c348e8a11ee962",
      "1a6dbf00b3e64e5bb664bfcb02338dc8",
      "e58bb8381ea744e6964d9b8385ed7fa5",
      "646b002426e04b6f8746d68708cf4929",
      "d332894b03794f949d7506bfbfdd9ef6",
      "e9423295c4bf4274b77a3255fa8fa875",
      "bb68e4d5f8ef4faba9edf3bfd8a6cec5",
      "2dba51dd830a436d85d2e38a5b0d4e0c",
      "eba2979f551c461bba5c2cfef9aed592",
      "28cdf19447ed4b2caa2532d5e03d34e0",
      "6fed36b9a49d4b6781f3287c55329abe",
      "f72f0e8255224bf2a6838248de24b2a0",
      "8d13b6d17a25457c84ef6417a0834bc1",
      "2155bf56e9a143e0ada9a3c8c018eaa2",
      "8af8df0d1e0f4ebfa468a5747dce9f8b",
      "0d12ef7a86184005b82ad808bc8709c6",
      "b322707232ac48b8a40d693785f05de5",
      "808c96a6a917438b86e613dba7262aec",
      "4c80bad1734d4de0908c636c8e73b142",
      "1e0940749bc64d749b367369983ff693",
      "73fa82a3900b4ff3b8c02c6e65ce8f5d",
      "b969c1e127704d06927693a577b65b47",
      "e34b39903a6041d6901385481be9dd78",
      "bfe3dd1ff5e54e6fb3df78eca3ab30de",
      "335946a21e0747289182e330950e41fe",
      "651344d9198e49c6a986fca6e9175a30",
      "7d7f7d7fd319447d8bfc168838358740",
      "d9118374729042b78228042c014b3e57",
      "75a4fd1b6d1a4aa1974f51d783454eb1",
      "603d5b813da74461b5f499e0cc36de05",
      "63528fbabf444ca2b396179fbdfbda43",
      "4b740c00d0894bc181b503e06af806b8",
      "f2540956d75049e28bb9be1d93273c26",
      "832e3f5b605c4ad1876d87e7648971c3",
      "00bf5f85e9dd476ca9f55bf6f4f66590",
      "6cbc773c0a1c43169cb37b6c253f6df5",
      "a64bee012b3a4e359760a1439d674b46",
      "782f26f7a75f458698c2550b88d0332b",
      "4f777207650442468a41172598052ec5",
      "c1108b566d9e4bf3a7d1b273bc4b93a1",
      "57cddbb1075249249f599c9361ec843f",
      "c61e13257ff145019f7cabc026a32054"
     ]
    },
    "id": "f5411e44",
    "outputId": "64e55a69-671e-4080-d12c-cf56fa12a536"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1ff710ac2347b5ade243b0d8bacb6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f959296a71d44da49bebf48162119c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795bf7d61227473a9ba67ebe744338d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8479fd7c2dbc47319329ca5e9351f263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d953f58c168845fabe9b5185e3c6b0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec514ade7add4498b722c77daaffc840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e65dd3829f4a8d9439a686865b865f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cdf19447ed4b2caa2532d5e03d34e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73fa82a3900b4ff3b8c02c6e65ce8f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b740c00d0894bc181b503e06af806b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded tiiuae/falcon-7b\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name_falcon = \"tiiuae/falcon-7b\"\n",
    "try:\n",
    "    tokenizer_falcon = AutoTokenizer.from_pretrained(model_name_falcon)\n",
    "    model_falcon = AutoModelForCausalLM.from_pretrained(model_name_falcon)\n",
    "    print(f\"Successfully loaded {model_name_falcon}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load {model_name_falcon}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y_LBGQGVrYTj",
    "outputId": "aaafe78f-b61f-481a-9aa8-50de95b933e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Generated Text Falcon\n",
      "User: Write a detailed article on how AI is transforming education in 2025.\n",
      "Assistant: How do you want me to do that?\n",
      "Writer: I don’t know. Just figure it out. I’ll be able to edit it later. You seem to be pretty good at that. It’s just a 3000-word article. The deadline is in a week. What is AI doing to education? You write about that, and I can edit and publish it. That’d be a big help. And you’re getting paid for it, too. Isn’that great? I know that you have a lot of work, but I really need your help right now. This is a bit of a deadline. Why don't you just write up some kind of outline? Or you could start the article, send it to me, I'll edit, publish, then you edit again and we're done. We'd both get paid and it would be great. Can you do it? Oh, thank you so much. Good luck with your assignment.\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=model_falcon, tokenizer=tokenizer_falcon)\n",
    "output = generator(prompt, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n",
    "generated_text = output[0]['generated_text']\n",
    "\n",
    "print(\"\\n\\n Generated Text Falcon\")\n",
    "print(generated_text) # Use print to display the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHwIO9ipYArb"
   },
   "source": [
    "**Evaluation of Text (Falcon):**\n",
    "The text had no relevance with the topic but more of a back and forth conversation dialogue which wasnt the main idea of the prompt. The vocabulary and grammar is good and the text adds a bit of humour but not in the right context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnGp_ZdxWX3f"
   },
   "source": [
    "## Evaluation Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFQeransWCFX"
   },
   "source": [
    "This is the Evaluation Table for the above three open source LLMs based on the output generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgnGhI0uV9Wk"
   },
   "source": [
    "| **Evaluation Aspect** | **Mistral** | **LlaMa3** | **Falcon** |\n",
    "| --------------------- | ----------- | ----------- | ----------- |\n",
    "| Task Relevance        | 5           | 3           | 1           |\n",
    "| Logical Flow          | 4           | 3           | 2           |\n",
    "| Originality           | 3           | 5           | 4           |\n",
    "| Language Quality      | 5           | 4           | 3           |\n",
    "| Accuracy of Content   | 4           | 2           | 2           |\n",
    "| **Overall Score**     | **21/25**   | **17/25**   | **12/25**   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4ndJZz4aJbU"
   },
   "source": [
    "Hence Mistral will be used for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVOY8PbVNT4K"
   },
   "source": [
    "## HELM (Holistic Evaluation of Language Models)\n",
    "The evaluation results indicate the mean win rates for the models tested:\n",
    "\n",
    "| **Model**            |**mean_win_rates**|\n",
    "|----------------------|------------------|\n",
    "|**Mistral v0.1 (7B)** | 0.884            |\n",
    "|**LLaMA 2 (7B)**      | 0.607            |\n",
    "|**Falcon (7B)**       | 0.378            |\n",
    "\n",
    "Based on these results, Mistral v0.1 (7B) shows the highest performance."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "e-CVx6_PvP8c",
    "AeWraT-9skRZ",
    "or-5taOHsu7-",
    "o5S1IpibsyA4"
   ],
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
